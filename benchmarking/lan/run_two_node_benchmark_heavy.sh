#!/usr/bin/env bash
# Copyright 2026 Siddharth Viswanathan
# This file was fully generated by Codex and may contain modifications by Siddharth Viswanathan as of 2026.
# SPDX-License-Identifier: Apache-2.0

set -euo pipefail

usage() {
  cat <<'EOF'
Usage:
  run_two_node_benchmark_heavy.sh --remote-url URL [options]

This script is run on the "driver" node (for example Ubuntu) and benchmarks a
heavier two-node LAN workflow:
  - Generate N files per iteration (N in [files-min, files-max])
  - File sizes are random in [size-min-kb, size-max-kb]
  - Append one heavy gossip operation per file (payload includes file content)
  - Measure sync + replication latency to the remote peer

Options:
  --remote-url URL      Remote daemon base URL (required), e.g. http://192.168.1.24:8787
  --repo PATH           Repository root on this machine (default: current directory)
  --vcs-bin PATH        Local vcs binary (default: <repo>/vcs)
  --iterations N        Measured iterations (default: 25)
  --warmup N            Warmup iterations (default: 5)
  --files-min N         Minimum file count per iteration (default: 10)
  --files-max N         Maximum file count per iteration (default: 20)
  --size-min-kb N       Minimum file size in KB (default: 1)
  --size-max-kb N       Maximum file size in KB (default: 10)
  --poll-ms N           Poll interval while waiting for replication (default: 25)
  --timeout-ms N        Timeout per replication sample (default: 20000)
  --mode MODE           replication mode: manual-sync or passive-daemon (default: manual-sync)
  --sync-limit N        Passed to vcs sync --limit (default: 256)
  --sync-rounds N       Passed to vcs sync --rounds (default: 6)
  --tag NAME            Optional tag suffix for result directory
  --out-root PATH       Output root (default: benchmarking/results/lan-heavy)
  --keep-work-files     Keep generated workload files in output directory
  -h, --help            Show this help

Examples:
  ./benchmarking/lan/run_two_node_benchmark_heavy.sh \
    --remote-url http://192.168.1.24:8787 \
    --iterations 30 --warmup 6 --mode manual-sync

  ./benchmarking/lan/run_two_node_benchmark_heavy.sh \
    --remote-url http://192.168.1.24:8787 \
    --files-min 12 --files-max 20 --size-min-kb 2 --size-max-kb 10
EOF
}

require_command() {
  if ! command -v "$1" >/dev/null 2>&1; then
    echo "error: required command not found: $1" >&2
    exit 1
  fi
}

now_ns() {
  local value
  value="$(date +%s%N 2>/dev/null || true)"
  if [[ "$value" =~ ^[0-9]+$ ]]; then
    printf '%s\n' "$value"
    return
  fi
  if [[ -n "${EPOCHREALTIME:-}" ]]; then
    local sec frac
    sec="${EPOCHREALTIME%.*}"
    frac="${EPOCHREALTIME#*.}"
    frac="${frac}000000000"
    frac="${frac:0:9}"
    printf '%s%s\n' "$sec" "$frac"
    return
  fi
  printf '%s000000000\n' "$(date +%s)"
}

elapsed_ms() {
  local start_ns="$1"
  local end_ns="$2"
  awk -v s="$start_ns" -v e="$end_ns" 'BEGIN { printf "%.3f", (e - s) / 1000000.0 }'
}

rand_between() {
  local min="$1"
  local max="$2"
  echo $((min + RANDOM % (max - min + 1)))
}

csv_escape() {
  local raw="$1"
  raw="${raw//\"/\"\"}"
  printf '"%s"' "$raw"
}

json_escape() {
  local raw="$1"
  raw="${raw//\\/\\\\}"
  raw="${raw//\"/\\\"}"
  raw="${raw//$'\n'/\\n}"
  raw="${raw//$'\r'/\\r}"
  raw="${raw//$'\t'/\\t}"
  printf '%s' "$raw"
}

json_number_or_null() {
  local raw="$1"
  if [[ -z "$raw" || "$raw" == "n/a" ]]; then
    printf 'null'
  else
    printf '%s' "$raw"
  fi
}

sha256_file() {
  local path="$1"
  if command -v sha256sum >/dev/null 2>&1; then
    sha256sum "$path" | awk '{print $1}'
    return
  fi
  if command -v shasum >/dev/null 2>&1; then
    shasum -a 256 "$path" | awk '{print $1}'
    return
  fi
  echo "error: neither sha256sum nor shasum is available" >&2
  exit 1
}

remote_summary_seq() {
  local remote_url="$1"
  local author_id="$2"
  local response compact escaped_author pattern match
  response="$(curl -fsS "${remote_url%/}/v1/node")"
  compact="$(printf '%s' "$response" | tr -d '\r\n')"
  escaped_author="$(printf '%s' "$author_id" | sed 's/[][\\.^$*+?{}|()]/\\&/g')"
  pattern="\"${escaped_author}\"[[:space:]]*:[[:space:]]*[0-9][0-9]*"
  match="$(printf '%s' "$compact" | grep -oE "$pattern" | head -n1 || true)"
  if [[ -z "$match" ]]; then
    printf '0\n'
    return
  fi
  printf '%s' "$match" | sed -E 's/.*:[[:space:]]*([0-9]+).*/\1/'
}

parse_append_output_field() {
  local output="$1"
  local field="$2"
  case "$field" in
    author)
      printf '%s\n' "$output" | sed -n 's/.* author=\([^ ]*\).*/\1/p' | tail -n1
      ;;
    seq)
      printf '%s\n' "$output" | sed -n 's/.* seq=\([0-9][0-9]*\).*/\1/p' | tail -n1
      ;;
    *)
      return 1
      ;;
  esac
}

compute_stats_from_csv_column() {
  local csv_path="$1"
  local column="$2"
  local prefix="$3"
  local values stats key value

  values="$(awk -F',' -v col="$column" 'NR > 1 && $1 == "measured" && $3 == "ok" { print $col + 0 }' "$csv_path" | sort -n)"
  stats="$(
    printf '%s\n' "$values" | awk '
      BEGIN {
        n = 0
        sum = 0
        sumsq = 0
      }
      NF > 0 {
        v = $1 + 0
        vals[++n] = v
        sum += v
        sumsq += v * v
      }
      END {
        if (n == 0) {
          print "count=0"
          print "mean=n/a"
          print "median=n/a"
          print "p95=n/a"
          print "stddev=n/a"
          print "min=n/a"
          print "max=n/a"
          exit
        }

        mean = sum / n
        if (n % 2 == 1) {
          median = vals[(n + 1) / 2]
        } else {
          median = (vals[n / 2] + vals[n / 2 + 1]) / 2
        }

        p95_idx = int((95 * n + 99) / 100)
        if (p95_idx < 1) {
          p95_idx = 1
        }
        if (p95_idx > n) {
          p95_idx = n
        }
        p95 = vals[p95_idx]

        var = (sumsq / n) - (mean * mean)
        if (var < 0) {
          var = 0
        }
        stddev = sqrt(var)

        printf "count=%d\n", n
        printf "mean=%.3f\n", mean
        printf "median=%.3f\n", median
        printf "p95=%.3f\n", p95
        printf "stddev=%.3f\n", stddev
        printf "min=%.3f\n", vals[1]
        printf "max=%.3f\n", vals[n]
      }
    '
  )"

  while IFS='=' read -r key value; do
    [[ -z "${key:-}" ]] && continue
    eval "${prefix}_${key}=\"${value}\""
  done <<< "$stats"
}

compute_min_avg_max_from_csv_column() {
  local csv_path="$1"
  local column="$2"
  local prefix="$3"
  local stats key value

  stats="$(
    awk -F',' -v col="$column" '
      NR > 1 && $1 == "measured" {
        v = $col + 0
        count++
        sum += v
        if (count == 1 || v < minv) minv = v
        if (count == 1 || v > maxv) maxv = v
      }
      END {
        if (count == 0) {
          print "min=n/a"
          print "avg=n/a"
          print "max=n/a"
          exit
        }
        printf "min=%.3f\n", minv
        printf "avg=%.3f\n", (sum / count)
        printf "max=%.3f\n", maxv
      }
    ' "$csv_path"
  )"

  while IFS='=' read -r key value; do
    [[ -z "${key:-}" ]] && continue
    eval "${prefix}_${key}=\"${value}\""
  done <<< "$stats"
}

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DEFAULT_OUT_ROOT="$SCRIPT_DIR/../results/lan-heavy"

REPO_PATH="$(pwd)"
VCS_BIN=""
REMOTE_URL=""
ITERATIONS=25
WARMUP=5
FILES_MIN=10
FILES_MAX=20
SIZE_MIN_KB=1
SIZE_MAX_KB=10
POLL_MS=25
TIMEOUT_MS=20000
MODE="manual-sync"
SYNC_LIMIT=256
SYNC_ROUNDS=6
RUN_TAG=""
OUT_ROOT="$DEFAULT_OUT_ROOT"
KEEP_WORK_FILES=0

while [[ $# -gt 0 ]]; do
  case "$1" in
    --remote-url)
      REMOTE_URL="$2"
      shift 2
      ;;
    --repo)
      REPO_PATH="$2"
      shift 2
      ;;
    --vcs-bin)
      VCS_BIN="$2"
      shift 2
      ;;
    --iterations)
      ITERATIONS="$2"
      shift 2
      ;;
    --warmup)
      WARMUP="$2"
      shift 2
      ;;
    --files-min)
      FILES_MIN="$2"
      shift 2
      ;;
    --files-max)
      FILES_MAX="$2"
      shift 2
      ;;
    --size-min-kb)
      SIZE_MIN_KB="$2"
      shift 2
      ;;
    --size-max-kb)
      SIZE_MAX_KB="$2"
      shift 2
      ;;
    --poll-ms)
      POLL_MS="$2"
      shift 2
      ;;
    --timeout-ms)
      TIMEOUT_MS="$2"
      shift 2
      ;;
    --mode)
      MODE="$2"
      shift 2
      ;;
    --sync-limit)
      SYNC_LIMIT="$2"
      shift 2
      ;;
    --sync-rounds)
      SYNC_ROUNDS="$2"
      shift 2
      ;;
    --tag)
      RUN_TAG="$2"
      shift 2
      ;;
    --out-root)
      OUT_ROOT="$2"
      shift 2
      ;;
    --keep-work-files)
      KEEP_WORK_FILES=1
      shift
      ;;
    -h|--help)
      usage
      exit 0
      ;;
    *)
      echo "error: unknown argument: $1" >&2
      usage
      exit 1
      ;;
  esac
done

if [[ -z "$REMOTE_URL" ]]; then
  echo "error: --remote-url is required" >&2
  usage
  exit 1
fi

if [[ -z "$VCS_BIN" ]]; then
  VCS_BIN="$REPO_PATH/vcs"
fi

if [[ "$MODE" != "manual-sync" && "$MODE" != "passive-daemon" ]]; then
  echo "error: --mode must be one of: manual-sync, passive-daemon" >&2
  exit 1
fi
if ((FILES_MIN < 1 || FILES_MAX < FILES_MIN)); then
  echo "error: invalid files range --files-min=$FILES_MIN --files-max=$FILES_MAX" >&2
  exit 1
fi
if ((SIZE_MIN_KB < 1 || SIZE_MAX_KB < SIZE_MIN_KB)); then
  echo "error: invalid size range --size-min-kb=$SIZE_MIN_KB --size-max-kb=$SIZE_MAX_KB" >&2
  exit 1
fi

if [[ ! -d "$REPO_PATH/.git" ]]; then
  echo "error: --repo must point to a git repository root: $REPO_PATH" >&2
  exit 1
fi
if [[ ! -x "$VCS_BIN" ]]; then
  echo "error: vcs binary is not executable: $VCS_BIN" >&2
  exit 1
fi

require_command curl
require_command grep
require_command sort
require_command awk
require_command sed
require_command tr
require_command dd
require_command base64
require_command tar

# Validate connectivity to remote daemon.
curl -fsS "${REMOTE_URL%/}/v1/node" >/dev/null

TIMESTAMP_UTC="$(date -u +"%Y%m%dT%H%M%SZ")"
if [[ -n "$RUN_TAG" ]]; then
  RUN_DIR="$OUT_ROOT/$TIMESTAMP_UTC-$RUN_TAG"
else
  RUN_DIR="$OUT_ROOT/$TIMESTAMP_UTC"
fi
mkdir -p "$RUN_DIR"
WORK_DIR="$RUN_DIR/work"
mkdir -p "$WORK_DIR"

CSV_OUT="$RUN_DIR/heavy_workflow.csv"
SUMMARY_JSON="$RUN_DIR/summary.json"
SUMMARY_MD="$RUN_DIR/summary.md"

POLL_SLEEP_SECONDS="$(awk -v ms="$POLL_MS" 'BEGIN { if (ms < 1) ms = 1; printf "%.3f", ms / 1000.0 }')"

echo "phase,iteration,status,file_count,total_file_bytes,total_payload_b64_bytes,generate_ms,append_ms,sync_ms,replication_ms,polls,error" >"$CSV_OUT"

echo "Heavy LAN benchmark configuration:"
echo "  repo:             $REPO_PATH"
echo "  vcs-bin:          $VCS_BIN"
echo "  remote-url:       $REMOTE_URL"
echo "  mode:             $MODE"
echo "  iterations:       $ITERATIONS"
echo "  warmup:           $WARMUP"
echo "  files range:      $FILES_MIN..$FILES_MAX"
echo "  size range (KB):  $SIZE_MIN_KB..$SIZE_MAX_KB"
echo "  poll-ms:          $POLL_MS"
echo "  timeout-ms:       $TIMEOUT_MS"
echo "  sync-limit:       $SYNC_LIMIT"
echo "  sync-rounds:      $SYNC_ROUNDS"
echo "  output-dir:       $RUN_DIR"
echo

TOTAL_RUNS=$((ITERATIONS + WARMUP))

for ((i = 1; i <= TOTAL_RUNS; i++)); do
  phase="warmup"
  if ((i > WARMUP)); then
    phase="measured"
  fi

  iter_dir="$WORK_DIR/iter-$i"
  rm -rf "$iter_dir"
  mkdir -p "$iter_dir/files"

  file_count="$(rand_between "$FILES_MIN" "$FILES_MAX")"
  total_file_bytes=0
  total_payload_b64_bytes=0

  generate_start_ns="$(now_ns)"
  for ((f = 1; f <= file_count; f++)); do
    size_kb="$(rand_between "$SIZE_MIN_KB" "$SIZE_MAX_KB")"
    size_bytes=$((size_kb * 1024))
    file_name="$(printf 'file_%02d.bin' "$f")"
    file_path="$iter_dir/files/$file_name"

    if ! dd if=/dev/urandom of="$file_path" bs="$size_bytes" count=1 2>/dev/null; then
      dd if=/dev/urandom of="$file_path" bs=1024 count="$size_kb" 2>/dev/null
    fi
    total_file_bytes=$((total_file_bytes + size_bytes))
  done
  generate_end_ns="$(now_ns)"
  generate_ms="$(elapsed_ms "$generate_start_ns" "$generate_end_ns")"

  append_start_ns="$(now_ns)"
  status="ok"
  author_id=""
  expected_seq=0
  append_err=""

  for file_path in "$iter_dir/files"/*; do
    [[ -f "$file_path" ]] || continue
    file_name="$(basename "$file_path")"
    file_size="$(wc -c < "$file_path" | tr -d '[:space:]')"
    file_sha="$(sha256_file "$file_path")"
    file_b64="$(base64 < "$file_path" | tr -d '\n')"
    b64_len="${#file_b64}"
    total_payload_b64_bytes=$((total_payload_b64_bytes + b64_len))

    payload="{\"benchmark\":\"lan.heavy.file\",\"iteration\":$i,\"file\":\"$file_name\",\"size_bytes\":$file_size,\"sha256\":\"$file_sha\",\"content_b64\":\"$file_b64\"}"

    if ! file_append_output="$(cd "$REPO_PATH" && "$VCS_BIN" op append --type bench.lan.heavy.file --data "$payload" 2>&1)"; then
      status="error"
      append_err="file op append failed for $file_name: $file_append_output"
      break
    fi
  done

  if [[ "$status" == "ok" ]]; then
    marker_payload="{\"benchmark\":\"lan.heavy.marker\",\"iteration\":$i,\"file_count\":$file_count,\"total_file_bytes\":$total_file_bytes,\"total_payload_b64_bytes\":$total_payload_b64_bytes}"
    if ! marker_output="$(cd "$REPO_PATH" && "$VCS_BIN" op append --type bench.lan.heavy.marker --data "$marker_payload" 2>&1)"; then
      status="error"
      append_err="marker op append failed: $marker_output"
    else
      author_id="$(parse_append_output_field "$marker_output" "author")"
      expected_seq="$(parse_append_output_field "$marker_output" "seq")"
      if [[ -z "$author_id" || -z "$expected_seq" ]]; then
        status="error"
        append_err="unable to parse author/seq from marker output: $marker_output"
      fi
    fi
  fi

  append_end_ns="$(now_ns)"
  append_ms="$(elapsed_ms "$append_start_ns" "$append_end_ns")"

  sync_ms="0.000"
  replication_ms="0.000"
  polls=0
  observed_seq=0

  if [[ "$status" == "ok" ]]; then
    replication_start_ns="$(now_ns)"

    if [[ "$MODE" == "manual-sync" ]]; then
      sync_start_ns="$(now_ns)"
      if ! sync_output="$(cd "$REPO_PATH" && "$VCS_BIN" sync --peer "$REMOTE_URL" --limit "$SYNC_LIMIT" --rounds "$SYNC_ROUNDS" 2>&1)"; then
        status="error"
        append_err="manual sync failed: $sync_output"
      fi
      sync_end_ns="$(now_ns)"
      sync_ms="$(elapsed_ms "$sync_start_ns" "$sync_end_ns")"
    fi

    if [[ "$status" == "ok" ]]; then
      timeout_ns=$((replication_start_ns + TIMEOUT_MS * 1000000))
      status="timeout"
      while true; do
        seq_candidate="$(remote_summary_seq "$REMOTE_URL" "$author_id" || true)"
        if [[ "$seq_candidate" =~ ^[0-9]+$ ]]; then
          observed_seq="$seq_candidate"
          if ((observed_seq >= expected_seq)); then
            status="ok"
            break
          fi
        fi

        now="$(now_ns)"
        if ((now >= timeout_ns)); then
          break
        fi

        polls=$((polls + 1))
        sleep "$POLL_SLEEP_SECONDS"
      done

      if [[ "$status" != "ok" ]]; then
        append_err="remote summary for $author_id did not reach seq $expected_seq before timeout"
      fi
    fi

    replication_end_ns="$(now_ns)"
    replication_ms="$(elapsed_ms "$replication_start_ns" "$replication_end_ns")"
  fi

  printf '%s,%d,%s,%d,%d,%d,%s,%s,%s,%s,%d,%s\n' \
    "$phase" \
    "$i" \
    "$status" \
    "$file_count" \
    "$total_file_bytes" \
    "$total_payload_b64_bytes" \
    "$generate_ms" \
    "$append_ms" \
    "$sync_ms" \
    "$replication_ms" \
    "$polls" \
    "$(csv_escape "$append_err")" >>"$CSV_OUT"

  if ((KEEP_WORK_FILES == 0)); then
    rm -rf "$iter_dir"
  fi
done

compute_stats_from_csv_column "$CSV_OUT" 7 "generate"
compute_stats_from_csv_column "$CSV_OUT" 8 "append"
compute_stats_from_csv_column "$CSV_OUT" 9 "sync"
compute_stats_from_csv_column "$CSV_OUT" 10 "replication"

measured_failures="$(awk -F',' 'NR > 1 && $1 == "measured" && $3 != "ok" { c++ } END { print c + 0 }' "$CSV_OUT")"

compute_min_avg_max_from_csv_column "$CSV_OUT" 4 "files"
compute_min_avg_max_from_csv_column "$CSV_OUT" 5 "bytes"
compute_min_avg_max_from_csv_column "$CSV_OUT" 6 "payload"

cat >"$SUMMARY_JSON" <<EOF
{
  "timestamp_utc": "$(json_escape "$TIMESTAMP_UTC")",
  "remote_url": "$(json_escape "$REMOTE_URL")",
  "mode": "$(json_escape "$MODE")",
  "iterations": $ITERATIONS,
  "warmup": $WARMUP,
  "files_min": $FILES_MIN,
  "files_max": $FILES_MAX,
  "size_min_kb": $SIZE_MIN_KB,
  "size_max_kb": $SIZE_MAX_KB,
  "poll_ms": $POLL_MS,
  "timeout_ms": $TIMEOUT_MS,
  "sync_limit": $SYNC_LIMIT,
  "sync_rounds": $SYNC_ROUNDS,
  "repo_path": "$(json_escape "$REPO_PATH")",
  "vcs_bin": "$(json_escape "$VCS_BIN")",
  "measured_failures": $measured_failures,
  "workload": {
    "file_count": {
      "min": $(json_number_or_null "$files_min"),
      "avg": $(json_number_or_null "$files_avg"),
      "max": $(json_number_or_null "$files_max")
    },
    "total_file_bytes": {
      "min": $(json_number_or_null "$bytes_min"),
      "avg": $(json_number_or_null "$bytes_avg"),
      "max": $(json_number_or_null "$bytes_max")
    },
    "total_payload_b64_bytes": {
      "min": $(json_number_or_null "$payload_min"),
      "avg": $(json_number_or_null "$payload_avg"),
      "max": $(json_number_or_null "$payload_max")
    }
  },
  "metrics_ms": {
    "generate": {
      "count": $generate_count,
      "mean": $(json_number_or_null "$generate_mean"),
      "median": $(json_number_or_null "$generate_median"),
      "p95": $(json_number_or_null "$generate_p95"),
      "stddev": $(json_number_or_null "$generate_stddev"),
      "min": $(json_number_or_null "$generate_min"),
      "max": $(json_number_or_null "$generate_max")
    },
    "append": {
      "count": $append_count,
      "mean": $(json_number_or_null "$append_mean"),
      "median": $(json_number_or_null "$append_median"),
      "p95": $(json_number_or_null "$append_p95"),
      "stddev": $(json_number_or_null "$append_stddev"),
      "min": $(json_number_or_null "$append_min"),
      "max": $(json_number_or_null "$append_max")
    },
    "sync": {
      "count": $sync_count,
      "mean": $(json_number_or_null "$sync_mean"),
      "median": $(json_number_or_null "$sync_median"),
      "p95": $(json_number_or_null "$sync_p95"),
      "stddev": $(json_number_or_null "$sync_stddev"),
      "min": $(json_number_or_null "$sync_min"),
      "max": $(json_number_or_null "$sync_max")
    },
    "replication": {
      "count": $replication_count,
      "mean": $(json_number_or_null "$replication_mean"),
      "median": $(json_number_or_null "$replication_median"),
      "p95": $(json_number_or_null "$replication_p95"),
      "stddev": $(json_number_or_null "$replication_stddev"),
      "min": $(json_number_or_null "$replication_min"),
      "max": $(json_number_or_null "$replication_max")
    }
  }
}
EOF

cat >"$SUMMARY_MD" <<EOF
# Two-Node LAN Heavy Benchmark Summary

- Timestamp (UTC): \`$TIMESTAMP_UTC\`
- Remote URL: \`$REMOTE_URL\`
- Mode: \`$MODE\`
- Iterations: \`$ITERATIONS\` (warmup \`$WARMUP\`)
- Workload files per iteration: \`$FILES_MIN..$FILES_MAX\`
- File size range: \`$SIZE_MIN_KB..$SIZE_MAX_KB KB\`
- Poll interval: \`$POLL_MS ms\`
- Timeout: \`$TIMEOUT_MS ms\`
- Sync params: \`limit=$SYNC_LIMIT\`, \`rounds=$SYNC_ROUNDS\`
- Measured failures: \`$measured_failures\`

## Workload Realized (Measured Iterations)

| Quantity | min | avg | max |
|---|---:|---:|---:|
| file_count | $files_min | $files_avg | $files_max |
| total_file_bytes | $bytes_min | $bytes_avg | $bytes_max |
| total_payload_b64_bytes | $payload_min | $payload_avg | $payload_max |

## Timing Metrics (ms, measured+ok samples only)

| Metric | samples | mean | median | p95 | stddev | min | max |
|---|---:|---:|---:|---:|---:|---:|---:|
| generate | $generate_count | $generate_mean | $generate_median | $generate_p95 | $generate_stddev | $generate_min | $generate_max |
| append | $append_count | $append_mean | $append_median | $append_p95 | $append_stddev | $append_min | $append_max |
| sync | $sync_count | $sync_mean | $sync_median | $sync_p95 | $sync_stddev | $sync_min | $sync_max |
| replication | $replication_count | $replication_mean | $replication_median | $replication_p95 | $replication_stddev | $replication_min | $replication_max |

## Artifacts

- Raw iteration data: \`$CSV_OUT\`
- JSON summary: \`$SUMMARY_JSON\`
- Markdown summary: \`$SUMMARY_MD\`
EOF

if ((KEEP_WORK_FILES == 0)); then
  rmdir "$WORK_DIR" 2>/dev/null || true
fi

echo
echo "Heavy benchmark complete."
echo "  raw data:       $CSV_OUT"
echo "  summary (json): $SUMMARY_JSON"
echo "  summary (md):   $SUMMARY_MD"
